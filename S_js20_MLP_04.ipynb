{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street 2020: Multi-Layer Perceptron IV\n",
    "\n",
    "Using MLP to classify\n",
    " - changes the MLP parameters, from previous 3 versions\n",
    " - Averaging over model fitted over different partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Summary, and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1; cuda available: True\n",
      "     active environment : base\n",
      "working directory: /home/AWC/wang/learn/kaggle/k_JaneStreet20\n"
     ]
    }
   ],
   "source": [
    "# Imports, environment, and paths\n",
    "import os, sys, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datatable as dtable\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('dark_background') #plt.style.use('default')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import janestreet\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "print(f'PyTorch version: {torch.__version__}; cuda available: {torch.cuda.is_available()}')\n",
    "\n",
    "# auxiliary --------------\n",
    "from importlib import reload\n",
    "import time\n",
    "\n",
    "# Print environment ------\n",
    "pd.set_option('display.max_columns', 200) \n",
    "!conda info | grep 'active environment' # or use: !conda info --envs | grep '*'\n",
    "print(f'working directory: {os.getcwd()}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducability\n",
    "# globalSeed=67\n",
    "# np.random.seed(globalSeed) # for reproducibility, does this work?\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppressing warning before saving the presentation, only\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Variables for later sections:\n",
    " - data, features: \n",
    " - nFeat, daySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 3.81 s, total: 1min 4s\n",
      "Wall time: 5.35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddir='~/learn/kaggle/Data/JaneStreet20' # local\n",
    "# ddir='../input/jane-street-market-prediction' # kaggle\n",
    "\n",
    "# data = pd.read_csv(os.path.join(ddir,\"train.csv\"))\n",
    "data = dtable.fread(os.path.join(ddir,\"train.csv\")).to_pandas() # using datatable for faster loading\n",
    "features = pd.read_csv(os.path.join(ddir,\"features.csv\"))\n",
    "\n",
    "nFeat=features.shape[0]\n",
    "featName=[f'feature_{n}' for n in range(nFeat)]\n",
    "xywCol=featName+['resp','weight']\n",
    "daySet=data['date'].unique()\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Preprecessing\n",
    "\n",
    "The data may be used in later sections are\n",
    " - dataBlock\n",
    " - data_t, data_v, data_c for TVT data\n",
    " - nFeat, featName\n",
    " - norm, for normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Deal with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def fillNanWithinDay(df,dayCol,fillCol,spanFillNa=1):\n",
    "    \"\"\"fill NaN within date\n",
    "    \n",
    "    This function does (forward) fill without crossing dates, using EMA of a trailing\n",
    "    window. Equal value in dayCol column indicates same date. \"Date\" here can\n",
    "    be generalized to block with equal dayCol value\n",
    "    Parameter:\n",
    "      df: dataframe, original data\n",
    "      dayCol: string, column name. Equal value indicates same date (block)\n",
    "      fillCol: list of straings, names of columns to fill NaN\n",
    "      spanFillNa: integer. Using a trailing ema of given span to fill NaN.\n",
    "          spanFillNa=1 is equivalent to 'ffill' of df.fillna()\n",
    "    return:\n",
    "      list of pd.DataFrame of day, NaN replaced\n",
    "    \"\"\"\n",
    "    dfList=[]\n",
    "    dayList=df[dayCol].unique()\n",
    "    for day in dayList:\n",
    "        data_1=df.loc[df['date']==day]\n",
    "        data_1_=data_1[fillCol].ewm(span=spanFillNa,adjust=False,ignore_na=True).mean()\n",
    "        data_1_fill=data_1.copy()\n",
    "        for cname in fillCol:\n",
    "            toFill=data_1[cname].isna()\n",
    "            data_1_fill.loc[toFill,cname]=data_1_.loc[toFill,cname]\n",
    "        dfList.append(data_1_fill)\n",
    "    \n",
    "    return pd.concat(dfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#original:  2390491\n",
      "#After dropping NaN and 0 wieghts:  1981287\n",
      "#Nan in train: 0, 0, 0\n",
      "CPU times: user 1min 14s, sys: 5.48 s, total: 1min 19s\n",
      "Wall time: 1min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fill NaN after first data points of day\n",
    "spanFillNa=3\n",
    "fillCol=[f'feature_{ii}' for ii in range(nFeat) if ii not in [0,64]] # features to fill nan\n",
    "data=fillNanWithinDay(data,'date',fillCol,spanFillNa=spanFillNa)\n",
    "print('#original: ',data.shape[0])\n",
    "\n",
    "# Fill NaN on beginning of day\n",
    "nPointStart=300 # num of samples to estimate day start\n",
    "dayStart=[data.loc[data['date']==day,fillCol].iloc[:nPointStart] for day in daySet]\n",
    "f_mean=pd.concat(dayStart).mean()\n",
    "data[fillCol] = data[fillCol].fillna(f_mean)\n",
    "f_mean.to_csv(os.path.join('fmean_JS20_MLP_01.csv')) # './model_sv'\n",
    "\n",
    "data=data.loc[~data[xywCol].isna().any(axis=1)]\n",
    "data=data.loc[data['weight']>0].reset_index(drop=True)  # Dropping 0 weight\n",
    "print('#After dropping NaN and 0 wieghts: ',data.shape[0])\n",
    "\n",
    "print('#Nan in train: {:d}, {:d}, {:d}'.format(data.loc[:,featName].isna().to_numpy().sum(),\n",
    "                                               data.loc[:,'resp'].isna().to_numpy().sum(),\n",
    "                                               data.loc[:,'weight'].isna().to_numpy().sum()))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traing MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm0 = torch.nn.BatchNorm1d(len(features))\n",
    "        self.dropout0 = torch.nn.Dropout(0.10143786981358652)\n",
    "\n",
    "        self.dense1 = torch.nn.Linear(len(features), 843)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(843)\n",
    "        self.dropout1 = torch.nn.Dropout(0.19720339053599725)\n",
    "\n",
    "        self.dense2 = torch.nn.Linear(843, 1724)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(1724)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2703017847244654)\n",
    "\n",
    "        self.dense3 = torch.nn.Linear(1724, 856)\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(856)\n",
    "        self.dropout3 = torch.nn.Dropout(0.23148340929571917)\n",
    "        \n",
    "        self.dense3a = torch.nn.Linear(856, 256)\n",
    "        self.batch_norm3a = torch.nn.BatchNorm1d(256)\n",
    "        self.dropout3a = torch.nn.Dropout(0.23148340929571917)\n",
    "        \n",
    "        self.dense4 = torch.nn.Linear(256, 90)\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(90)\n",
    "        self.dropout4 = torch.nn.Dropout(0.2357768967777311)\n",
    "        \n",
    "        self.dense5 = torch.nn.Linear(90, 512)\n",
    "        self.batch_norm5 = torch.nn.BatchNorm1d(512)\n",
    "        self.dropout5 = torch.nn.Dropout(0.2357768967777311)\n",
    "\n",
    "        self.dense6 = torch.nn.Linear(512, 394)\n",
    "        self.batch_norm6 = torch.nn.BatchNorm1d(394)\n",
    "        self.dropout6 = torch.nn.Dropout(0.2357768967777311)\n",
    "        \n",
    "        self.dense7 = torch.nn.Linear(394, 64)\n",
    "        self.batch_norm7 = torch.nn.BatchNorm1d(64)\n",
    "        self.dropout7 = torch.nn.Dropout(0.2357768967777311)\n",
    "\n",
    "        self.dense_out = torch.nn.Linear(64, 1)\n",
    "\n",
    "        self.Relu = torch.nn.ReLU(inplace=True)\n",
    "        self.PReLU = torch.nn.PReLU()\n",
    "        self.LeakyReLU = torch.nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.GeLU = torch.nn.GELU()\n",
    "        self.RReLU = torch.nn.RReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        #x = x * torch.sigmoid(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        #x = x * torch.sigmoid(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.dense3a(x)\n",
    "        x = self.batch_norm3a(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout3a(x)\n",
    "        \n",
    "        x = self.dense4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        #x = x * torch.sigmoid(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        x = self.dense5(x)\n",
    "        x = self.batch_norm5(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        x = self.dense6(x)\n",
    "        x = self.batch_norm6(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout6(x)\n",
    "        \n",
    "        x = self.dense7(x)\n",
    "        x = self.batch_norm7(x)\n",
    "        x=self.Relu(x)\n",
    "        x = self.dropout7(x)\n",
    "\n",
    "        x = self.dense_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Trainging: helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketDataset:\n",
    "    def __init__(self, data, featName):\n",
    "        self.features = data[featName].values\n",
    "        self.label = (data['resp']>0).astype('int').values.reshape(-1, 1)\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features = data['features'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        features = data['features'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds).reshape(-1)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score: #  + self.delta\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            # ema.apply_shadow()\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            # ema.restore()\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_score_bincount(date, weight, resp, action):\n",
    "    count_i = len(np.unique(date))\n",
    "    # print('weight: ', weight)\n",
    "    # print('resp: ', resp)\n",
    "    # print('action: ', action)\n",
    "    # print('weight * resp * action: ', weight * resp * action)\n",
    "    Pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n",
    "    u = np.clip(t, 0, 6) * np.sum(Pi)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0, train_loss:0.69363, u_score:2790.90463, auc:0.53376, logloss:0.69099, time: 1.13min\n",
      "Validation score improved (-inf --> 0.5337639595205583). Saving model!\n",
      "EPOCH:  1, train_loss:0.69108, u_score:2215.05566, auc:0.52651, logloss:0.69161, time: 2.27min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  2, train_loss:0.69036, u_score:2888.33461, auc:0.53609, logloss:0.69099, time: 3.30min\n",
      "Validation score improved (0.5337639595205583 --> 0.5360893426695515). Saving model!\n",
      "EPOCH:  3, train_loss:0.68955, u_score:2418.58550, auc:0.53006, logloss:0.69222, time: 4.32min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  4, train_loss:0.68870, u_score:1882.15471, auc:0.53325, logloss:0.69220, time: 5.41min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "EPOCH:  5, train_loss:0.68774, u_score:1345.42814, auc:0.52950, logloss:0.69328, time: 6.42min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stop!\n",
      "EPOCH:  0, train_loss:0.69341, u_score:708.23301, auc:0.52429, logloss:0.69278, time: 7.53min\n",
      "Validation score improved (-inf --> 0.5242931420850732). Saving model!\n",
      "EPOCH:  1, train_loss:0.69067, u_score:957.80284, auc:0.52704, logloss:0.69219, time: 8.57min\n",
      "Validation score improved (0.5242931420850732 --> 0.5270362337754997). Saving model!\n",
      "EPOCH:  2, train_loss:0.68986, u_score:1435.82227, auc:0.52959, logloss:0.69174, time: 9.61min\n",
      "Validation score improved (0.5270362337754997 --> 0.529593283398052). Saving model!\n",
      "EPOCH:  3, train_loss:0.68903, u_score:1666.30279, auc:0.52782, logloss:0.69206, time: 10.66min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  4, train_loss:0.68824, u_score:1094.79526, auc:0.52320, logloss:0.69269, time: 11.66min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "EPOCH:  5, train_loss:0.68715, u_score:1780.57350, auc:0.52691, logloss:0.69276, time: 12.75min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4096\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ckp_path = os.path.join('JS20_MLP_01.pth') # './model_sv'\n",
    "\n",
    "# gkf = GroupKFold(n_splits = 5)\n",
    "gss = GroupShuffleSplit(n_splits=2, test_size=0.2)\n",
    "for fold, (idx_t, idx_v) in enumerate(gss.split(data, groups=data['date'])):\n",
    "    # print(f'fold {fold}: ',len(idx_t),len(idx_v),len(idx_t)+len(idx_v))\n",
    "    \n",
    "    train_set = MarketDataset(data.loc[idx_t],featName)\n",
    "    train_loader=DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_set = MarketDataset(data.loc[idx_v],featName)\n",
    "    valid_loader=DataLoader(valid_set, batch_size=batch_size, shuffle=False) # Using True is bad, why??????????\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n",
    "    es = EarlyStopping(patience=3, mode=\"max\")\n",
    "\n",
    "    ckp_path = f'./JS20_MLP_03_{fold}.pth'\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n",
    "        valid_pred = inference_fn(model, valid_loader, device)\n",
    "        auc_score = roc_auc_score((data.loc[idx_v,'resp']>0).astype(int).values.reshape(-1, 1), valid_pred)\n",
    "        logloss_score = log_loss((data.loc[idx_v,'resp']>0).astype(int).values.reshape(-1, 1), valid_pred)\n",
    "        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n",
    "\n",
    "        u_score = utility_score_bincount(date=data.loc[idx_v,'date'].values, weight=data.loc[idx_v,'weight'].values,\n",
    "                                         resp=data.loc[idx_v,'resp'].values, action=valid_pred)\n",
    "        print(f\"EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n",
    "              f\"time: {(time.time() - start_time) / 60:.2f}min\")\n",
    "\n",
    "        es(auc_score, model, model_path=ckp_path)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "    #break # only train 1 model for fast, you can remove it to train 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-layer, out 64, 0.5352562415345534\n",
    "\n",
    "# EPOCH:  0, train_loss:0.69303, u_score:1893.28075, auc:0.53431, logloss:0.69105, time: 0.98min\n",
    "# Validation score improved (-inf --> 0.5343079813321263). Saving model!\n",
    "# EPOCH:  1, train_loss:0.69098, u_score:1851.42244, auc:0.53393, logloss:0.69070, time: 1.92min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  2, train_loss:0.69036, u_score:2282.55294, auc:0.53530, logloss:0.69092, time: 2.90min\n",
    "# Validation score improved (0.5343079813321263 --> 0.5353037631791487). Saving model!\n",
    "# EPOCH:  3, train_loss:0.68976, u_score:1254.54635, auc:0.53207, logloss:0.69152, time: 3.82min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  4, train_loss:0.68896, u_score:1847.79352, auc:0.53551, logloss:0.69125, time: 4.73min\n",
    "# Validation score improved (0.5353037631791487 --> 0.5355135928614407). Saving model!\n",
    "# EPOCH:  5, train_loss:0.68802, u_score:1195.19258, auc:0.52963, logloss:0.69251, time: 5.64min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  6, train_loss:0.68698, u_score:1947.54415, auc:0.53320, logloss:0.69375, time: 6.56min\n",
    "# EarlyStopping counter: 2 out of 3\n",
    "# EPOCH:  7, train_loss:0.68582, u_score:1040.02801, auc:0.53028, logloss:0.69368, time: 7.47min\n",
    "# EarlyStopping counter: 3 out of 3\n",
    "# Early stop!\n",
    "# EPOCH:  0, train_loss:0.69310, u_score:1436.89301, auc:0.52977, logloss:0.69170, time: 8.48min\n",
    "# Validation score improved (-inf --> 0.5297706227692168). Saving model!\n",
    "# EPOCH:  1, train_loss:0.69095, u_score:2089.00796, auc:0.53099, logloss:0.69155, time: 9.39min\n",
    "# Validation score improved (0.5297706227692168 --> 0.530992434417539). Saving model!\n",
    "# EPOCH:  2, train_loss:0.69036, u_score:1823.46750, auc:0.53243, logloss:0.69111, time: 10.30min\n",
    "# Validation score improved (0.530992434417539 --> 0.5324334397090253). Saving model!\n",
    "# EPOCH:  3, train_loss:0.68977, u_score:1622.69886, auc:0.53193, logloss:0.69152, time: 11.25min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  4, train_loss:0.68906, u_score:2796.98194, auc:0.53313, logloss:0.69145, time: 12.27min\n",
    "# Validation score improved (0.5324334397090253 --> 0.533128426189544). Saving model!\n",
    "# EPOCH:  5, train_loss:0.68833, u_score:933.43726, auc:0.53177, logloss:0.69190, time: 13.23min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  6, train_loss:0.68747, u_score:1012.29392, auc:0.53054, logloss:0.69257, time: 14.15min\n",
    "# EarlyStopping counter: 2 out of 3\n",
    "# EPOCH:  7, train_loss:0.68658, u_score:2523.13760, auc:0.53415, logloss:0.69323, time: 15.08min\n",
    "# Validation score improved (0.533128426189544 --> 0.5341527543536294). Saving model!\n",
    "# EPOCH:  8, train_loss:0.68564, u_score:2020.05890, auc:0.53667, logloss:0.69334, time: 16.03min\n",
    "# Validation score improved (0.5341527543536294 --> 0.5366674144960986). Saving model!\n",
    "# EPOCH:  9, train_loss:0.68450, u_score:1575.12710, auc:0.53293, logloss:0.69441, time: 16.95min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# for i in range(2): # for fast inference, you can change 1-->5 to get higher score\n",
    "#     torch.cuda.empty_cache()\n",
    "#     device = torch.device(\"cuda:0\")\n",
    "#     model = Model()\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     ckp_path = f'./JS20_MLP_03_{fold}.pth'\n",
    "#     model.load_state_dict(torch.load(ckp_path))\n",
    "#     models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try not to use GPU here ******\n",
    "\n",
    "# env = janestreet.make_env()\n",
    "# env_iter = env.iter_test()\n",
    "\n",
    "# alpha=2/(spanFillNa+1)\n",
    "# prevDate=None\n",
    "# opt_th=0.5\n",
    "# nTest=0\n",
    "# for (test_df_1, pred_df) in tqdm(env_iter):\n",
    "#     test_df=test_df_1.iloc[0]\n",
    "#     # Update fill value\n",
    "#     if prevDate!=test_df['date']:\n",
    "#         xx_fill=test_df[fillCol].fillna(f_mean)\n",
    "#     else:\n",
    "#         xx_fill=(((1-alpha)*xx_fill+alpha*test_df[fillCol].fillna(0))*(~test_df[fillCol].isna()) +\n",
    "#                  xx_fill*test_df[fillCol].isna() )\n",
    "#     if xx_fill.isna().any():\n",
    "#         print('xx_fill contains NaN'); break\n",
    "    \n",
    "#     if test_df['weight'].item() > 0:\n",
    "#         xx=test_df.loc[featName].copy()\n",
    "#         if xx[fillCol].isna().any():\n",
    "#             xx[fillCol]=test_df[fillCol].fillna(xx_fill)\n",
    "#         for i, clf in enumerate(models):\n",
    "#             if i == 0:\n",
    "#                 pred=clf(torch.tensor(np.expand_dims(xx.values,axis=0),dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy()\n",
    "#             else:\n",
    "#                 pred+=clf(torch.tensor(np.expand_dims(xx.values,axis=0),dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy()\n",
    "#         pred/=len(models)\n",
    "#         pred_df.action=np.where(pred >= opt_th, 1, 0).astype(int)\n",
    "#     else:\n",
    "#         pred_df.action = 0\n",
    "#     env.predict(pred_df)\n",
    "#     prevDate=test_df['date']\n",
    "#     nTest+=1\n",
    "# print(f'nn={nTest}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
