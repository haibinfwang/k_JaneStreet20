{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street 2020: Multi-Layer Perceptron II\n",
    "\n",
    "Using MLP to classify\n",
    " - Feature engineering: adding ema of features\n",
    " - Training:\n",
    "   * MLP Model\n",
    "   * Cost function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Summary, and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1; cuda available: True\n",
      "     active environment : base\n",
      "working directory: /home/AWC/wang/learn/kaggle/k_JaneStreet20\n"
     ]
    }
   ],
   "source": [
    "# Imports, environment, and paths\n",
    "import os, sys, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datatable as dtable\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('dark_background') #plt.style.use('default')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "print(f'PyTorch version: {torch.__version__}; cuda available: {torch.cuda.is_available()}')\n",
    "\n",
    "# auxiliary --------------\n",
    "from importlib import reload\n",
    "import time\n",
    "\n",
    "# Print environment ------\n",
    "pd.set_option('display.max_columns', 200) \n",
    "!conda info | grep 'active environment' # or use: !conda info --envs | grep '*'\n",
    "print(f'working directory: {os.getcwd()}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducability\n",
    "# globalSeed=67\n",
    "# np.random.seed(globalSeed) # for reproducibility, does this work?\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppressing warning before saving the presentation, only\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Variables for later sections:\n",
    " - data, features: \n",
    " - nFeat, daySet, featName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.8 s, sys: 3.44 s, total: 43.3 s\n",
      "Wall time: 3.97 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddir='~/learn/kaggle/Data/JaneStreet20' # local\n",
    "#ddir='../input/jane-street-market-prediction' # kaggle\n",
    "\n",
    "# data = pd.read_csv(os.path.join(ddir,\"train.csv\"))\n",
    "data = dtable.fread(os.path.join(ddir,\"train.csv\")).to_pandas() # using datatable for faster loading\n",
    "features = pd.read_csv(os.path.join(ddir,\"features.csv\"))\n",
    "\n",
    "nFeat=features.shape[0]\n",
    "featName=[f'feature_{n}' for n in range(nFeat)]\n",
    "xywCol=featName+['resp','weight']\n",
    "daySet=data['date'].unique()\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Preprecessing\n",
    "\n",
    "The data may be used in later sections are\n",
    " - dataBlock\n",
    " - data_t, data_v, data_c for TVT data\n",
    " - nFeat, featName\n",
    " - norm, for normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Deal with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def fillNanWithinDay(df,dayCol,fillCol,spanFillNa=1):\n",
    "    \"\"\"fill NaN within date\n",
    "    \n",
    "    This function does (forward) fill without crossing dates, using EMA of a trailing\n",
    "    window. Equal value in dayCol column indicates same date. \"Date\" here can\n",
    "    be generalized to block with equal dayCol value\n",
    "    Parameter:\n",
    "      df: dataframe, original data\n",
    "      dayCol: string, column name. Equal value indicates same date (block)\n",
    "      fillCol: list of straings, names of columns to fill NaN\n",
    "      spanFillNa: integer. Using a trailing ema of given span to fill NaN.\n",
    "          spanFillNa=1 is equivalent to 'ffill' of df.fillna()\n",
    "    return:\n",
    "      list of pd.DataFrame of day, NaN replaced\n",
    "    \"\"\"\n",
    "    dfList=[]\n",
    "    dayList=df[dayCol].unique()\n",
    "    for day in dayList:\n",
    "        data_1=df.loc[df['date']==day]\n",
    "        data_1_=data_1[fillCol].ewm(span=spanFillNa,adjust=False,ignore_na=True).mean()\n",
    "        data_1_fill=data_1.copy()\n",
    "        for cname in fillCol:\n",
    "            toFill=data_1[cname].isna()\n",
    "            data_1_fill.loc[toFill,cname]=data_1_.loc[toFill,cname]\n",
    "        dfList.append(data_1_fill)\n",
    "    \n",
    "    return pd.concat(dfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0=data.loc[data['date']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>4.427147</td>\n",
       "      <td>-0.248016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>6.196915</td>\n",
       "      <td>-1.089747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>3.017005</td>\n",
       "      <td>-1.155526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_126  feature_127\n",
       "3253     4.427147    -0.248016\n",
       "3254     6.196915    -1.089747\n",
       "3255     3.017005    -1.155526\n",
       "3256          NaN          NaN\n",
       "3257          NaN          NaN\n",
       "3258          NaN          NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col=['feature_126','feature_127']\n",
    "toFill=data_0[col].isna()\n",
    "\n",
    "# toFill.idxmax()\n",
    "data_0.loc[3253:3258,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_126</th>\n",
       "      <th>feature_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>3.370767</td>\n",
       "      <td>-0.300797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3254</th>\n",
       "      <td>4.783841</td>\n",
       "      <td>-0.695272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3255</th>\n",
       "      <td>3.900423</td>\n",
       "      <td>-0.925399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>3.900423</td>\n",
       "      <td>-0.925399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>3.900423</td>\n",
       "      <td>-0.925399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>3.900423</td>\n",
       "      <td>-0.925399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_126  feature_127\n",
       "3253     3.370767    -0.300797\n",
       "3254     4.783841    -0.695272\n",
       "3255     3.900423    -0.925399\n",
       "3256     3.900423    -0.925399\n",
       "3257     3.900423    -0.925399\n",
       "3258     3.900423    -0.925399"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanFillNa=3\n",
    "data_1=data_0[col].ewm(span=spanFillNa,adjust=False,ignore_na=True).mean()\n",
    "data_1.loc[3253:3258]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_126    0.618023\n",
      "feature_127    7.448875\n",
      "Name: 1, dtype: float64\n",
      "feature_126    0.618023\n",
      "feature_127    7.448875\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "idx=1\n",
    "print(data_1.loc[idx])\n",
    "print(data_2.loc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_126    0.0\n",
       "feature_127    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data_1-data_2).abs().sum(skipna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_126    21\n",
       "feature_127    21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0[col].isna().sum(skipna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_126    0\n",
       "feature_127    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2.isna().sum(axis=0,skipna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_126    3256\n",
       "feature_127    3256\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2.isna().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_126    0.0\n",
       "feature_127    0.0\n",
       "Name: 3258, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2.loc[3258]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 0 ns, total: 10.7 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "alpha=2/(spanFillNa+1)\n",
    "data_2=pd.DataFrame(0,index=data_1.index,columns=data_1.columns)\n",
    "for idxRow,row in data_0[col].iterrows():\n",
    "    if idxRow==0:\n",
    "        rowLast=row\n",
    "    else:\n",
    "        rowLast=(((1-alpha)*rowLast+alpha*row.fillna(0)) * (~row.isna()) + rowLast*row.isna() )\n",
    "                 \n",
    "    data_2.loc[idxRow]=rowLast\n",
    "    if rowLast.isna().any():\n",
    "        print('na'); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_126    3.900423\n",
       "feature_127   -0.925399\n",
       "dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((1-alpha)*rowLast+alpha*row.fillna(0)) * (~row.isna()) + rowLast*row.isna() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#original:  2390491\n",
      "#After dropping NaN and 0 wieghts:  1981287\n",
      "#Nan in train: 0, 0, 0\n",
      "CPU times: user 1min 10s, sys: 4.12 s, total: 1min 14s\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fill NaN after first data points of day\n",
    "fillCol=[f'feature_{ii}' for ii in range(nFeat) if ii not in [0,64]] # features to fill nan\n",
    "data=fillNanWithinDay(data,'date',fillCol,spanFillNa=3)\n",
    "print('#original: ',data.shape[0])\n",
    "\n",
    "# Fill NaN on beginning of day\n",
    "nPointStart=300 # num of samples to estimate day start\n",
    "dayStart=[data.loc[data['date']==day,fillCol].iloc[:nPointStart] for day in daySet]\n",
    "f_mean=pd.concat(dayStart).mean()\n",
    "data[fillCol] = data[fillCol].fillna(f_mean)\n",
    "f_mean.to_csv(os.path.join('./model_sv','fmean_JS20_MLP_01.csv'))\n",
    "\n",
    "data=data.loc[~data[xywCol].isna().any(axis=1)]\n",
    "data=data.loc[data['weight']>0].reset_index(drop=True)  # Dropping 0 weight\n",
    "print('#After dropping NaN and 0 wieghts: ',data.shape[0])\n",
    "\n",
    "print('#Nan in train: {:d}, {:d}, {:d}'.format(data.loc[:,featName].isna().to_numpy().sum(),\n",
    "                                               data.loc[:,'resp'].isna().to_numpy().sum(),\n",
    "                                               data.loc[:,'weight'].isna().to_numpy().sum()))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train-validation-test (TVT) split, and cast to binary classification\n",
    "\n",
    "Cast the problem to binary classficiation proble. That is, The real-valued 'resp' is cast to {0,1}.\n",
    "\n",
    "Split will be performed in the following way\n",
    " - Split is performed on dates\n",
    " - Reserve a latest segment of days as test\n",
    " - Train-validation split is randomized selection of days\n",
    " - In training and validation, each row is considered independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Nan in train: 0, 0, 0\n",
      "#Nan in train: 0, 0, 0\n"
     ]
    }
   ],
   "source": [
    "# # 1.2 TVT split, and cast resp to y{0, 1}\n",
    "# gss = GroupShuffleSplit(n_splits=1, test_size=0.2) #  random_state=2\n",
    "# idx_t, idx_v = next(gss.split(data, groups=data['date']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.3 Centering and scaling: *not* in use\n",
    "\n",
    "Centering and scaling is done on train set. Same scheme is applied to validation and test sets.\n",
    " - Feature 0 and 64 is not scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "# featToNorm=[f'feature_{ii}' for ii in range(nFeat) if ii not in [0,64]]\n",
    "# norm=pd.DataFrame({'c':pd.Series(0,index=featName), 's':pd.Series(1,index=featName) }).T\n",
    "# norm.loc['c',featToNorm]=data_t['X'][featToNorm].mean()\n",
    "# norm.loc['s',featToNorm]=data_t['X'][featToNorm].std()\n",
    "\n",
    "# data_t['X']=(data_t['X']-norm.loc['c'])/norm.loc['s']\n",
    "# data_v['X']=(data_v['X']-norm.loc['c'])/norm.loc['s']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Verify proper normalization\n",
    "# _, axs = plt.subplots(nrows=1, ncols=2,figsize=(16, 3))\n",
    "# data_t['X'][featToNorm].mean().plot(ax=axs[0])\n",
    "# data_v['X'][featToNorm].mean().plot(ax=axs[0])\n",
    "# axs[0].set(ylabel='mean')\n",
    "# data_t['X'][featToNorm].std().plot(ax=axs[1])\n",
    "# data_v['X'][featToNorm].std().plot(ax=axs[1])\n",
    "# axs[1].set(ylabel='std');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traing MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm0 = torch.nn.BatchNorm1d(len(features))\n",
    "        self.dropout0 = torch.nn.Dropout(0.10143786981358652)\n",
    "\n",
    "        hidden_size = 256\n",
    "        self.dense1 = torch.nn.Linear(len(features), 384)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(384)\n",
    "        self.dropout1 = torch.nn.Dropout(0.19720339053599725)\n",
    "\n",
    "        self.dense2 = torch.nn.Linear(384, 896)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(896)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2703017847244654)\n",
    "\n",
    "        self.dense3 = torch.nn.Linear(896, 896)\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(896)\n",
    "        self.dropout3 = torch.nn.Dropout(0.23148340929571917)\n",
    "\n",
    "        self.dense4 = torch.nn.Linear(896, 394)\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(394)\n",
    "        self.dropout4 = torch.nn.Dropout(0.2357768967777311)\n",
    "\n",
    "        self.dense5 = torch.nn.Linear(394, 1)\n",
    "\n",
    "        self.Relu = torch.nn.ReLU(inplace=True)\n",
    "        self.PReLU = torch.nn.PReLU()\n",
    "        self.LeakyReLU = torch.nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.GeLU = torch.nn.GELU()\n",
    "        self.RReLU = torch.nn.RReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.dense4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.dense5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Trainging: helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketDataset:\n",
    "    def __init__(self, data, featName):\n",
    "        self.features = data[featName].values\n",
    "        self.label = (data['resp']>0).astype('int').values.reshape(-1, 1)\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features = data['features'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        features = data['features'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds).reshape(-1)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score: #  + self.delta\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            # ema.apply_shadow()\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            # ema.restore()\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_score_bincount(date, weight, resp, action):\n",
    "    count_i = len(np.unique(date))\n",
    "    # print('weight: ', weight)\n",
    "    # print('resp: ', resp)\n",
    "    # print('action: ', action)\n",
    "    # print('weight * resp * action: ', weight * resp * action)\n",
    "    Pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n",
    "    u = np.clip(t, 0, 6) * np.sum(Pi)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0, train_loss:0.69388, u_score:1246.02201, auc:0.52964, logloss:0.69140, time: 0.95min\n",
      "Validation score improved (-inf --> 0.529644626795742). Saving model!\n",
      "EPOCH:  1, train_loss:0.69082, u_score:1487.25101, auc:0.53333, logloss:0.69140, time: 1.89min\n",
      "Validation score improved (0.529644626795742 --> 0.5333268874827157). Saving model!\n",
      "EPOCH:  2, train_loss:0.69010, u_score:1513.15369, auc:0.53031, logloss:0.69197, time: 2.82min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  3, train_loss:0.68933, u_score:834.25969, auc:0.53124, logloss:0.69228, time: 3.77min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "EPOCH:  4, train_loss:0.68840, u_score:1589.11385, auc:0.53404, logloss:0.69219, time: 4.72min\n",
      "Validation score improved (0.5333268874827157 --> 0.5340436295219545). Saving model!\n",
      "EPOCH:  5, train_loss:0.68743, u_score:1734.98346, auc:0.53099, logloss:0.69339, time: 5.62min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  6, train_loss:0.68666, u_score:796.98737, auc:0.52877, logloss:0.69546, time: 6.51min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "EPOCH:  7, train_loss:0.68571, u_score:858.96949, auc:0.52822, logloss:0.69537, time: 7.41min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4096\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2) #  random_state=2\n",
    "idx_t, idx_v = next(gss.split(data, groups=data['date']))\n",
    "\n",
    "train_set = MarketDataset(data.loc[idx_t],featName)\n",
    "train_loader=DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_set = MarketDataset(data.loc[idx_v],featName)\n",
    "valid_loader=DataLoader(valid_set, batch_size=batch_size, shuffle=False) # Using True is bad, why??????????\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = Model()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n",
    "\n",
    "ckp_path = os.path.join('./model_sv','JS20_MLP_01.pth')\n",
    "\n",
    "es = EarlyStopping(patience=3, mode=\"max\")\n",
    "for epoch in range(10):\n",
    "    train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n",
    "    valid_pred = inference_fn(model, valid_loader, device)\n",
    "    auc_score = roc_auc_score((data.loc[idx_v,'resp']>0).astype(int).values.reshape(-1, 1), valid_pred)\n",
    "    logloss_score = log_loss((data.loc[idx_v,'resp']>0).astype(int).values.reshape(-1, 1), valid_pred)\n",
    "    valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n",
    "\n",
    "    u_score = utility_score_bincount(date=data.loc[idx_v,'date'].values, weight=data.loc[idx_v,'weight'].values,\n",
    "                                     resp=data.loc[idx_v,'resp'].values, action=valid_pred)\n",
    "    print(f\"EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n",
    "          f\"time: {(time.time() - start_time) / 60:.2f}min\")\n",
    "\n",
    "    es(auc_score, model, model_path=ckp_path)\n",
    "    if es.early_stop:\n",
    "        print(\"Early stop!\")\n",
    "        break\n",
    "#     break # only train 1 model for fast, you can remove it to train 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predict on test set\n",
    "\n",
    " - o wieght rows does not appear to have much impact (small, worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "# Load test set\n",
    "# Predict in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH:  0, train_loss:0.69401, u_score:1256.95984, auc:0.52760, logloss:0.69205, time: 1.12min\n",
    "# Validation score improved (-inf --> 0.5276042643733097). Saving model!\n",
    "# EPOCH:  1, train_loss:0.69051, u_score:1226.78192, auc:0.52978, logloss:0.69214, time: 2.19min\n",
    "# Validation score improved (0.5276042643733097 --> 0.5297800412247824). Saving model!\n",
    "# EPOCH:  2, train_loss:0.68943, u_score:968.38580, auc:0.52706, logloss:0.69382, time: 3.26min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  3, train_loss:0.68845, u_score:972.59923, auc:0.52657, logloss:0.69465, time: 4.35min\n",
    "# EarlyStopping counter: 2 out of 3\n",
    "# EPOCH:  4, train_loss:0.68738, u_score:1267.13899, auc:0.52882, logloss:0.69441, time: 5.49min\n",
    "# EarlyStopping counter: 3 out of 3\n",
    "# Early stop!\n",
    "\n",
    "\n",
    "# Run 1:\n",
    "# EPOCH:  0, train_loss:0.69384, u_score:617.65718, auc:0.52557, logloss:0.69230, time: 0.86min\n",
    "# Validation score improved (-inf --> 0.5255701715967822). Saving model!\n",
    "# EPOCH:  1, train_loss:0.69121, u_score:545.33534, auc:0.53062, logloss:0.69306, time: 1.72min\n",
    "# Validation score improved (0.5255701715967822 --> 0.5306179912922396). Saving model!\n",
    "# EPOCH:  2, train_loss:0.69048, u_score:526.00783, auc:0.52900, logloss:0.69320, time: 2.57min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  3, train_loss:0.68964, u_score:528.03468, auc:0.53532, logloss:0.69383, time: 3.43min\n",
    "# Validation score improved (0.5306179912922396 --> 0.5353218402403408). Saving model!\n",
    "# EPOCH:  4, train_loss:0.68918, u_score:542.65725, auc:0.53162, logloss:0.69245, time: 4.29min\n",
    "# EarlyStopping counter: 1 out of 3\n",
    "# EPOCH:  5, train_loss:0.68864, u_score:155.54247, auc:0.52435, logloss:0.69569, time: 5.15min\n",
    "# EarlyStopping counter: 2 out of 3\n",
    "# EPOCH:  6, train_loss:0.68809, u_score:374.29251, auc:0.52604, logloss:0.69363, time: 6.01min\n",
    "# EarlyStopping counter: 3 out of 3\n",
    "# Early stop!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
