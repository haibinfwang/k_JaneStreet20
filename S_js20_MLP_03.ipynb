{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street 2020: Multi-Layer Perceptron I\n",
    "\n",
    "Using MLP to classify, the focus here is to establish the workflow and testing submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Summary, and initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1; cuda available: True\n",
      "     active environment : base\n",
      "working directory: /home/AWC/wang/learn/kaggle/k_JaneStreet20\n"
     ]
    }
   ],
   "source": [
    "# Imports, environment, and paths\n",
    "import os, sys, gc, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datatable as dtable\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, log_loss\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('dark_background') #plt.style.use('default')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import janestreet\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "print(f'PyTorch version: {torch.__version__}; cuda available: {torch.cuda.is_available()}')\n",
    "\n",
    "# auxiliary --------------\n",
    "from importlib import reload\n",
    "import time\n",
    "\n",
    "# Print environment ------\n",
    "pd.set_option('display.max_columns', 200) \n",
    "!conda info | grep 'active environment' # or use: !conda info --envs | grep '*'\n",
    "print(f'working directory: {os.getcwd()}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducability\n",
    "# globalSeed=67\n",
    "# np.random.seed(globalSeed) # for reproducibility, does this work?\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppressing warning before saving the presentation, only\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Variables for later sections:\n",
    " - data, features: \n",
    " - nFeat, daySet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.9 s, sys: 3.17 s, total: 43.1 s\n",
      "Wall time: 3.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddir='~/learn/kaggle/Data/JaneStreet20' # local\n",
    "# ddir='../input/jane-street-market-prediction' # kaggle\n",
    "\n",
    "# data = pd.read_csv(os.path.join(ddir,\"train.csv\"))\n",
    "data = dtable.fread(os.path.join(ddir,\"train.csv\")).to_pandas() # using datatable for faster loading\n",
    "features = pd.read_csv(os.path.join(ddir,\"features.csv\"))\n",
    "\n",
    "nFeat=features.shape[0]\n",
    "featName=[f'feature_{n}' for n in range(nFeat)]\n",
    "xywCol=featName+['resp','weight']\n",
    "daySet=data['date'].unique()\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Preprecessing\n",
    "\n",
    "The data may be used in later sections are\n",
    " - dataBlock\n",
    " - data_t, data_v, data_c for TVT data\n",
    " - nFeat, featName\n",
    " - norm, for normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Deal with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def fillNanWithinDay(df,dayCol,fillCol,spanFillNa=1):\n",
    "    \"\"\"fill NaN within date\n",
    "    \n",
    "    This function does (forward) fill without crossing dates, using EMA of a trailing\n",
    "    window. Equal value in dayCol column indicates same date. \"Date\" here can\n",
    "    be generalized to block with equal dayCol value\n",
    "    Parameter:\n",
    "      df: dataframe, original data\n",
    "      dayCol: string, column name. Equal value indicates same date (block)\n",
    "      fillCol: list of straings, names of columns to fill NaN\n",
    "      spanFillNa: integer. Using a trailing ema of given span to fill NaN.\n",
    "          spanFillNa=1 is equivalent to 'ffill' of df.fillna()\n",
    "    return:\n",
    "      list of pd.DataFrame of day, NaN replaced\n",
    "    \"\"\"\n",
    "    dfList=[]\n",
    "    dayList=df[dayCol].unique()\n",
    "    for day in dayList:\n",
    "        data_1=df.loc[df['date']==day]\n",
    "        data_1_=data_1[fillCol].ewm(span=spanFillNa,adjust=False,ignore_na=True).mean()\n",
    "        data_1_fill=data_1.copy()\n",
    "        for cname in fillCol:\n",
    "            toFill=data_1[cname].isna()\n",
    "            data_1_fill.loc[toFill,cname]=data_1_.loc[toFill,cname]\n",
    "        dfList.append(data_1_fill)\n",
    "    \n",
    "    return pd.concat(dfList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#original:  2390491\n",
      "#After dropping NaN and 0 wieghts:  1981287\n",
      "#Nan in train: 0, 0, 0\n",
      "CPU times: user 1min 11s, sys: 5.19 s, total: 1min 16s\n",
      "Wall time: 1min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Fill NaN after first data points of day\n",
    "spanFillNa=3\n",
    "fillCol=[f'feature_{ii}' for ii in range(nFeat) if ii not in [0,64]] # features to fill nan\n",
    "data=fillNanWithinDay(data,'date',fillCol,spanFillNa=spanFillNa)\n",
    "print('#original: ',data.shape[0])\n",
    "\n",
    "# Fill NaN on beginning of day\n",
    "nPointStart=300 # num of samples to estimate day start\n",
    "dayStart=[data.loc[data['date']==day,fillCol].iloc[:nPointStart] for day in daySet]\n",
    "f_mean=pd.concat(dayStart).mean()\n",
    "data[fillCol] = data[fillCol].fillna(f_mean)\n",
    "f_mean.to_csv(os.path.join('fmean_JS20_MLP_01.csv')) # './model_sv'\n",
    "\n",
    "data=data.loc[~data[xywCol].isna().any(axis=1)]\n",
    "data=data.loc[data['weight']>0].reset_index(drop=True)  # Dropping 0 weight\n",
    "print('#After dropping NaN and 0 wieghts: ',data.shape[0])\n",
    "\n",
    "print('#Nan in train: {:d}, {:d}, {:d}'.format(data.loc[:,featName].isna().to_numpy().sum(),\n",
    "                                               data.loc[:,'resp'].isna().to_numpy().sum(),\n",
    "                                               data.loc[:,'weight'].isna().to_numpy().sum()))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traing MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm0 = torch.nn.BatchNorm1d(len(features))\n",
    "        self.dropout0 = torch.nn.Dropout(0.10143786981358652)\n",
    "\n",
    "        hidden_size = 256\n",
    "        self.dense1 = torch.nn.Linear(len(features), 384)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(384)\n",
    "        self.dropout1 = torch.nn.Dropout(0.19720339053599725)\n",
    "\n",
    "        self.dense2 = torch.nn.Linear(384, 896)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(896)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2703017847244654)\n",
    "\n",
    "        self.dense3 = torch.nn.Linear(896, 896)\n",
    "        self.batch_norm3 = torch.nn.BatchNorm1d(896)\n",
    "        self.dropout3 = torch.nn.Dropout(0.23148340929571917)\n",
    "\n",
    "        self.dense4 = torch.nn.Linear(896, 394)\n",
    "        self.batch_norm4 = torch.nn.BatchNorm1d(394)\n",
    "        self.dropout4 = torch.nn.Dropout(0.2357768967777311)\n",
    "\n",
    "        self.dense5 = torch.nn.Linear(394, 1)\n",
    "\n",
    "        self.Relu = torch.nn.ReLU(inplace=True)\n",
    "        self.PReLU = torch.nn.PReLU()\n",
    "        self.LeakyReLU = torch.nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "        # self.GeLU = torch.nn.GELU()\n",
    "        self.RReLU = torch.nn.RReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm0(x)\n",
    "        x = self.dropout0(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.dense2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.dense4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = x * torch.sigmoid(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.dense5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Trainging: helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketDataset:\n",
    "    def __init__(self, data, featName):\n",
    "        self.features = data[featName].values\n",
    "        self.label = (data['resp']>0).astype('int').values.reshape(-1, 1)\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.features[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.label[idx], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        features = data['features'].to(device)\n",
    "        label = data['label'].to(device)\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        final_loss += loss.item()\n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for data in dataloader:\n",
    "        features = data['features'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "\n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds).reshape(-1)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model, model_path):\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "        else:\n",
    "            score = np.copy(epoch_score)\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "        elif score < self.best_score: #  + self.delta\n",
    "            self.counter += 1\n",
    "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            # ema.apply_shadow()\n",
    "            self.save_checkpoint(epoch_score, model, model_path)\n",
    "            # ema.restore()\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model, model_path):\n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "        self.val_score = epoch_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_score_bincount(date, weight, resp, action):\n",
    "    count_i = len(np.unique(date))\n",
    "    # print('weight: ', weight)\n",
    "    # print('resp: ', resp)\n",
    "    # print('action: ', action)\n",
    "    # print('weight * resp * action: ', weight * resp * action)\n",
    "    Pi = np.bincount(date, weight * resp * action)\n",
    "    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n",
    "    u = np.clip(t, 0, 6) * np.sum(Pi)\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0, train_loss:0.69375, u_score:549.96260, auc:0.53127, logloss:0.69133, time: 1.14min\n",
      "Validation score improved (-inf --> 0.5312697680032075). Saving model!\n",
      "EPOCH:  1, train_loss:0.69078, u_score:1618.14633, auc:0.53105, logloss:0.69198, time: 2.08min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  2, train_loss:0.69008, u_score:850.53158, auc:0.53296, logloss:0.69135, time: 3.03min\n",
      "Validation score improved (0.5312697680032075 --> 0.5329647523830431). Saving model!\n",
      "EPOCH:  3, train_loss:0.68934, u_score:2036.61191, auc:0.52846, logloss:0.69269, time: 3.98min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  4, train_loss:0.68843, u_score:1732.54685, auc:0.52972, logloss:0.69342, time: 4.92min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "EPOCH:  5, train_loss:0.68738, u_score:1306.18572, auc:0.52766, logloss:0.69420, time: 5.86min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stop!\n",
      "EPOCH:  0, train_loss:0.69388, u_score:3113.17045, auc:0.52854, logloss:0.69173, time: 6.83min\n",
      "Validation score improved (-inf --> 0.5285441965739621). Saving model!\n",
      "EPOCH:  1, train_loss:0.69079, u_score:2761.73450, auc:0.52783, logloss:0.69229, time: 7.78min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  2, train_loss:0.68991, u_score:3720.87763, auc:0.52908, logloss:0.69252, time: 8.71min\n",
      "Validation score improved (0.5285441965739621 --> 0.5290765823413337). Saving model!\n",
      "EPOCH:  3, train_loss:0.68900, u_score:2844.19349, auc:0.52514, logloss:0.69477, time: 9.65min\n",
      "EarlyStopping counter: 1 out of 3\n",
      "EPOCH:  4, train_loss:0.68803, u_score:3100.66603, auc:0.52701, logloss:0.69421, time: 10.59min\n",
      "EarlyStopping counter: 2 out of 3\n",
      "EPOCH:  5, train_loss:0.68712, u_score:3295.08805, auc:0.52640, logloss:0.69589, time: 11.54min\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stop!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4096\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# ckp_path = os.path.join('JS20_MLP_01.pth') # './model_sv'\n",
    "\n",
    "# gkf = GroupKFold(n_splits = 5)\n",
    "gss = GroupShuffleSplit(n_splits=2, test_size=0.2)\n",
    "for fold, (idx_t, idx_v) in enumerate(gss.split(data, groups=data['date'])):\n",
    "    # print(f'fold {fold}: ',len(idx_t),len(idx_v),len(idx_t)+len(idx_v))\n",
    "    \n",
    "    train_set = MarketDataset(data.loc[idx_t],featName)\n",
    "    train_loader=DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    valid_set = MarketDataset(data.loc[idx_v],featName)\n",
    "    valid_loader=DataLoader(valid_set, batch_size=batch_size, shuffle=False) # Using True is bad, why??????????\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n",
    "    es = EarlyStopping(patience=3, mode=\"max\")\n",
    "\n",
    "    ckp_path = f'./JS20_MLP_03_{fold}.pth'\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n",
    "        valid_pred = inference_fn(model, valid_loader, device)\n",
    "        auc_score = roc_auc_score((data.loc[idx_v,'resp']>0).astype(int).values.reshape(-1, 1), valid_pred)\n",
    "        logloss_score = log_loss((data.loc[idx_v,'resp']>0).astype(int).values.reshape(-1, 1), valid_pred)\n",
    "        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n",
    "\n",
    "        u_score = utility_score_bincount(date=data.loc[idx_v,'date'].values, weight=data.loc[idx_v,'weight'].values,\n",
    "                                         resp=data.loc[idx_v,'resp'].values, action=valid_pred)\n",
    "        print(f\"EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n",
    "              f\"time: {(time.time() - start_time) / 60:.2f}min\")\n",
    "\n",
    "        es(auc_score, model, model_path=ckp_path)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stop!\")\n",
    "            break\n",
    "    #break # only train 1 model for fast, you can remove it to train 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(2): # for fast inference, you can change 1-->5 to get higher score\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    ckp_path = f'./JS20_MLP_03_{fold}.pth'\n",
    "    model.load_state_dict(torch.load(ckp_path))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try not to use GPU here ******\n",
    "\n",
    "env = janestreet.make_env()\n",
    "env_iter = env.iter_test()\n",
    "\n",
    "alpha=2/(spanFillNa+1)\n",
    "prevDate=None\n",
    "opt_th=0.5\n",
    "nTest=0\n",
    "for (test_df_1, pred_df) in tqdm(env_iter):\n",
    "    test_df=test_df_1.iloc[0]\n",
    "    # Update fill value\n",
    "    if prevDate!=test_df['date']:\n",
    "        xx_fill=test_df[fillCol].fillna(f_mean)\n",
    "    else:\n",
    "        xx_fill=(((1-alpha)*xx_fill+alpha*test_df[fillCol].fillna(0))*(~test_df[fillCol].isna()) +\n",
    "                 xx_fill*test_df[fillCol].isna() )\n",
    "    if xx_fill.isna().any():\n",
    "        print('xx_fill contains NaN'); break\n",
    "    \n",
    "    if test_df['weight'].item() > 0:\n",
    "        xx=test_df.loc[featName].copy()\n",
    "        if xx[fillCol].isna().any():\n",
    "            xx[fillCol]=test_df[fillCol].fillna(xx_fill)\n",
    "        for i, clf in enumerate(models):\n",
    "            if i == 0:\n",
    "                pred=clf(torch.tensor(np.expand_dims(xx.values,axis=0),dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy()\n",
    "            else:\n",
    "                pred+=clf(torch.tensor(np.expand_dims(xx.values,axis=0),dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy()\n",
    "        pred/=len(models)\n",
    "        pred_df.action=np.where(pred >= opt_th, 1, 0).astype(int)\n",
    "    else:\n",
    "        pred_df.action = 0\n",
    "    env.predict(pred_df)\n",
    "    prevDate=test_df['date']\n",
    "    nTest+=1\n",
    "print(f'nn={nTest}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
